
spark-shell --master local[2] --jars ~/software/mysql-connector-java-5.1.27-bin.jar
spark-sql --master local[2] --jars ~/software/mysql-connector-java-5.1.27-bin.jar

spark-sql操作hive表数据

spark.table("show tables").show  xxxxx

spark.sql("show tables").show

//操作hive表，由于hive表存储在hdfs上，所以必须打开hdfs(启动hadoop)
spark.table("emp").show

spark.sql("select deptno,count(1) from emp group by deptno") .show
spark.sql("select deptno,count(1) from emp group by deptno").filter("deptno is not null").show


//saveAsTable不能写成saveastable，否则会报下面的错误
scala>  spark.sql("select deptno,count(1) from emp group by deptno").filter("deptno is not null").write.saveastable("hive_table_kobekun")
<console>:24: error: value saveastable is not a member of org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]
        spark.sql("select deptno,count(1) from emp group by deptno").filter("deptno is not null").write.saveastable("hive_table_kobekun")

spark.sql("select deptno,count(1) from emp group by deptno").filter("deptno is not null").write.saveAsTable("hive_table_kobekun")
报下面异常
org.apache.spark.sql.AnalysisException: Attribute name "count(1)" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.;

spark.sql("select deptno,count(1) as mount from emp group by deptno").filter("deptno is not null").write.saveAsTable("hive_table_kobekun")


spark.sqlContext.setConf("spark.sql.shuffle.partitions","10")
spark.sqlContext.getConf("spark.sql.shuffle.partitions")

spark.sql("select deptno,count(1) as mount from emp group by deptno").filter("deptno is not null").write.saveAsTable("hive_table_kobekun2")

在生产环境中，一定要设置spark.sql.shuffle.partitions，默认200




操作mysql数据

spark-shell:

val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/hive").option("dbtable", "hive.TBLS").option("user", "root").option("password", "root").load()

如果报下面的错误
no suitable driver
用
spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/hive").option("dbtab
le", "hive.TBLS").option("user", "root").option("driver","com.mysql.jdbc.Driver").option("password", "root").load()


jdbcDF.printSchema
jdbcDF.show
jdbcDF.select("TBL_ID","TBL_NAME").show

import java.util.Properties
val connectionProperties = new Properties()
connectionProperties.put("user", "root")
connectionProperties.put("password", "root")
connectionProperties.put("driver","com.mysql.jdbc.Driver")

val jdbcDF2 = spark.read.jdbc("jdbc:mysql://localhost:3306/", "hive.TBLS", connectionProperties)

jdbcDF2.show

spark-sql:

CREATE TEMPORARY VIEW jdbcTable
USING org.apache.spark.sql.jdbc
OPTIONS (
  url "jdbc:mysql://localhost:3306/",
  dbtable "hive.TBLS",
  user 'root',
  password 'root',
  driver 'com.mysql.jdbc.Driver'
);

注意：上面的语句执行时最后要加上;

select * from jdbcTable

hive和mysql综合使用

进入mysql，执行下面

create database spark;
use spark;

CREATE TABLE DEPT(
DEPTNO int(2) PRIMARY KEY,
DNAME VARCHAR(14) ,
LOC VARCHAR(13) ) ;

INSERT INTO DEPT VALUES(10,'ACCOUNTING','NEW YORK');
INSERT INTO DEPT VALUES(20,'RESEARCH','DALLAS');
INSERT INTO DEPT VALUES(30,'SALES','CHICAGO');
INSERT INTO DEPT VALUES(40,'OPERATIONS','BOSTON');


