
	

sqoop  ==》 SQL to hadoop   :  RDBMS <==> hadoop
	场景：1)数据在mysql中，想用hive进行处理
		  2) hive 统计分析好了，数据如何导入到mysql中
				
		统计结果最终是通过可视化报表展示
			HS2
			hive统计结果导出到RDBMS
	
	解决方案：
		MR
		==》抽象成常用的工具
			RDBMS：URL、driver、db、table、user、password
			HDFS：path
			hive：database、table、partition
	
	两大版本：
		1.4.* Sqoop1  常用
		1.99.* Sqoop2
		两个完全不同的框架
	
	sqoop1：职责   数据从RDBMS和hadoop之间进行导入导出操作
		底层是MR实现
		map  只有map
		reduce 没有reduce
		
		 基于hadoop作为参考点
		导入 RDBMS =》hadoop
		导出 hadoop =》RDBMS
		
	部署
		1、下载 cdh5  wget https://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.12.2.tar.gz
		2、解压  tar -zxvf sqoop-1.4.6-cdh5.12.2.tar.gz -C ~/app/
		3、配置环境变量  
			export SQOOP_HOME=/home/kobekun/app/sqoop-1.4.6-cdh5.12.2
			export PATH=$SQOOP_HOME/bin:$PATH
			source ~/.bashrc
	    4、配置文件：$SQOOP_HOME/conf/
			cp  sqoop-env-template.sh sqoop-env.sh
			export HADOOP_COMMON_HOME=/home/kobekun/app/hadoop-2.6.0-cdh5.12.2
			export HADOOP_MAPRED_HOME=/home/kobekun/app/hadoop-2.6.0-cdh5.12.2
			export HIVE_HOME=/home/kobekun/app/hive-1.1.0-cdh5.12.2
			
		5、将mysql驱动包放到sqoop的lib目录下
		
	sqoop map 默认是是4   splits=4
	
	sqoop help
	sqoop help 	list-databases
	
	查看数据库
	sqoop list-databases \
	--connect jdbc:mysql://ruozedata001:3306 \
	--password kobekun \
	--username root
	查看表
	sqoop list-tables \
	--connect jdbc:mysql://ruozedata001:3306/ruozedata_hive \
	--password kobekun \
	--username root
	
	表的数据导入到hdfs上**********************************************
	sqoop help import
	------------------------
	CREATE TABLE `emp`  (
  `empno` int,
  `ename` varchar(10) ,
  `job` varchar(9) ,
  `mgr` int,
  `hiredate` datetime(0) NULL DEFAULT NULL,
  `sal` double,
  `comm` double,
  `deptno` int
) ENGINE = InnoDB CHARACTER SET = utf8;

INSERT INTO `emp` VALUES (7369, 'SMITH', 'CLERK', 7902, '1980-12-17 00:00:00', 800.00, NULL, 20);
INSERT INTO `emp` VALUES (7499, 'ALLEN', 'SALESMAN', 7698, '1981-02-20 00:00:00', 1600.00, 300.00, 30);
INSERT INTO `emp` VALUES (7521, 'WARD', 'SALESMAN', 7698, '1981-02-22 00:00:00', 1250.00, 500.00, 30);
INSERT INTO `emp` VALUES (7566, 'JONES', 'MANAGER', 7839, '1981-04-02 00:00:00', 2975.00, NULL, 20);
INSERT INTO `emp` VALUES (7654, 'MARTIN', 'SALESMAN', 7698, '1981-09-28 00:00:00', 1250.00, 1400.00, 30);
INSERT INTO `emp` VALUES (7698, 'BLAKE', 'MANAGER', 7839, '1981-05-01 00:00:00', 2850.00, NULL, 30);
INSERT INTO `emp` VALUES (7782, 'CLARK', 'MANAGER', 7839, '1981-06-09 00:00:00', 2450.00, NULL, 10);
INSERT INTO `emp` VALUES (7788, 'SCOTT', 'ANALYST', 7566, '1982-12-09 00:00:00', 3000.00, NULL, 20);
INSERT INTO `emp` VALUES (7839, 'KING', 'PRESIDENT', NULL, '1981-11-17 00:00:00', 5000.00, NULL, 10);
INSERT INTO `emp` VALUES (7844, 'TURNER', 'SALESMAN', 7698, '1981-09-08 00:00:00', 1500.00, 0.00, 30);
INSERT INTO `emp` VALUES (7876, 'ADAMS', 'CLERK', 7788, '1983-01-12 00:00:00', 1100.00, NULL, 20);
INSERT INTO `emp` VALUES (7900, 'JAMES', 'CLERK', 7698, '1981-12-03 00:00:00', 950.00, NULL, 30);
INSERT INTO `emp` VALUES (7902, 'FORD', 'ANALYST', 7566, '1981-12-03 00:00:00', 3000.00, NULL, 20);
INSERT INTO `emp` VALUES (7934, 'MILLER', 'CLERK', 7782, '1982-01-23 00:00:00', 1300.00, NULL, 10);
	-----------------------------
	
	create database company;
	create table company.staff(
		id int(4) primary key not null auto_increment, 
		name varchar(255), 
		sex varchar(255)
	);
	insert into company.staff(name, sex) values('Thomas', 'Male');
	insert into company.staff(name, sex) values('Catalina', 'FeMale');
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/company \
--password kobekun \
--username root \
--table staff \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
-m 1

19/12/24 22:44:37 INFO mapreduce.JobSubmitter: number of splits:1

map默认是4  mapreduce.JobSubmitter: number of splits:2   ==》小于4条记录  几个记录就是几个splits
	
	hadoop fs -ls 和hadoop fs -ls /的区别  
	hadoop fs -ls  == hadoop fs -ls /user/当前用户名
	
	SELECT t.* FROM `staff` AS t LIMIT 1 看表存在不

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
--target-dir emp \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
-m 1
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
--columns "empno,ename,job,sal,comm" \
--target-dir emp_column_where \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
--where 'sal>2000' \
-m 1	
	
query和table不能同时使用
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
--target-dir emp_column_query \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
--query "select * from emp where empno>=7900 and \$CONDITIONS" \
-m 1	
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
--target-dir join \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
--query "select e.empno,e.ename,e.deptno,d.dname from emp e join dept d on e.deptno=d.deptno and \$CONDITIONS" \
-m 1
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp \
--password kobekun \
--username root \
--mapreduce-job-name FromMysql2HDFS \
--target-dir emp_append \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
--incremental append \
--check-column empno \
--last-value 7788 \
-m 1

mysql中记录
mysql> select * from emp;
+-------+--------+-----------+------+---------------------+------+------+--------+
| empno | ename  | job       | mgr  | hiredate            | sal  | comm | deptno |
+-------+--------+-----------+------+---------------------+------+------+--------+
|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 00:00:00 |  800 | NULL |     20 |
|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 00:00:00 | 1600 |  300 |     30 |
|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 00:00:00 | 1250 |  500 |     30 |
|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 00:00:00 | 2975 | NULL |     20 |
|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 00:00:00 | 1250 | 1400 |     30 |
|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 00:00:00 | 2850 | NULL |     30 |
|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 00:00:00 | 2450 | NULL |     10 |
|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 00:00:00 | 3000 | NULL |     20 |
|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 00:00:00 | 5000 | NULL |     10 |
|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 00:00:00 | 1500 |    0 |     30 |
|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 00:00:00 | 1100 | NULL |     20 |
|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 00:00:00 |  950 | NULL |     30 |
|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 00:00:00 | 3000 | NULL |     20 |
|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 00:00:00 | 1300 | NULL |     10 |
+-------+--------+-----------+------+---------------------+------+------+--------

hdfs中记录
[kobekun@ruozedata001 ~]$ hadoop fs -text  emp_append/par*
19/12/29 10:20:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
7839    KING    PRESIDENT       0       1981-11-17 00:00:00.0   5000.0  0       10
7844    TURNER  SALESMAN        7698    1981-09-08 00:00:00.0   1500.0  0.0     30
7876    ADAMS   CLERK   7788    1983-01-12 00:00:00.0   1100.0  0       20
7900    JAMES   CLERK   7698    1981-12-03 00:00:00.0   950.0   0       30
7902    FORD    ANALYST 7566    1981-12-03 00:00:00.0   3000.0  0       20
7934    MILLER  CLERK   7782    1982-01-23 00:00:00.0   1300.0  0       10
[kobekun@ruozedata001 ~]$ 

展示
sqoop eval \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--query "select * from emp where deptno=10" 
	
更新


This transfer can be faster! Use the --direct


--direct?能够提速呢？  SQOOP官网	oracle mysql都支持该参数

	When you provide a connect string to Sqoop, it inspects the protocol scheme to determine appropriate vendor-specific logic to use. 
	If Sqoop knows about a given database, it will work automatically. If not, you may need to specify the driver class to load via --driver. 
	This will use a generic code path which will use standard SQL to access the database.
	Sqoop provides some databases with faster, non-JDBC-based access mechanisms. These can be enabled by specfying the --direct parameter.
	
	当您向Sqoop提供连接字符串时，它将检查协议方案以确定要使用的适当的特定于供应商的逻辑。 如果Sqoop知道给定的数据库，它将自动运行。 
	如果不是，则可能需要指定要通过--driver加载的驱动程序类。 这将使用通用代码路径，该路径将使用标准SQL访问数据库。 
	Sqoop为某些数据库提供了更快的，****非基于JDBC的访问机制****。 可以通过指定--direct参数启用这些功能。
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp \
--password kobekun \
--username root \
--mapreduce-job-name FromMysql2HDFS \
--target-dir emp_direct \
--fields-terminated-by '\t' \
--null-string '' \
--null-non-string '0' \
--direct \
-m 1

----------------------------------------------------
create table salgrade (
     grade int,
     losal double,
     hisal double
);
 
insert into salgrade values (1, 700, 1200);
insert into salgrade values (2, 1201, 1400);
insert into salgrade values (3, 1401, 2000);
insert into salgrade values (4, 2001, 3000);
insert into salgrade values (5, 3001, 9999);
insert into salgrade values (6, 10000, 15000);
insert into salgrade values (7, 15001, 20000);
insert into salgrade values (8, 20001, 25000);
insert into salgrade values (9, 25001, 30000);
----------------------------------------------------
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table salgrade \
--password kobekun \
--username root \
-m 1	

如果不指定map=1，会报错，默认map=4，因为没有主键，map拆分的话都没有key可以拆分，所以会报错
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table salgrade \
--delete-target-dir \
--password kobekun \
--username root 

报错：ERROR tool.ImportTool: Import failed: No primary key could be found for table salgrade. 
Please specify one with --split-by or perform a sequential import with '-m 1'.	
	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table salgrade \
--delete-target-dir \
--password kobekun \
--username root \
--split-by 'grade'	



我发现 除了表里面5条记录的，hdfs文件是5个以外，其他都是4个文件

和条数没有直接关系

是和记录数你指定的字段的值有关系

这是inputformat的切片机制


sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp \
--delete-target-dir \
--target-dir emp_options_file \
--password kobekun \
--username root \
--m 2	
------------------------------------------------
import 
--connect
jdbc:mysql://ruozedata001:3306/sqoop
--table
emp 
--delete-target-dir
--target-dir
emp_options_file
--password
kobekun 
--username
root 
--m
2	
-----------------------------------------------

把上面内容复制到emp.opt中  vi emp.opt

sqoop --options-file ./emp.opt

上面操作之前，新增emp表的主键
ALTER TABLE `emp` ADD PRIMARY KEY ( `empno` );

hdfs导出到MySQL****************************************************

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--mapreduce-job-name FromMysql2HDFS \
--target-dir emp \
-m 1
--------------------------------------------------
sqoop export \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp_demo \
--export-dir /user/kobekun/emp \
--password kobekun \
--username root \
--m 1	

aaaa
---------------------------------------------------
执行上面操作报错说表不存在，在mysql中先创建表
create table emp_demo as select * from emp where 1=2;

导出默认分隔符是"，"

sqoop export \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp_demo \
--export-dir /user/kobekun/emp_column_split \
--columns "empno,ename,job,sal,comm" \
--fields-terminated-by '\t' \
--password kobekun \
--username root \
--m 1

批量导出

sqoop export \
--Dsqoop.export.records.per.statement=10 \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp_demo \
--export-dir /user/kobekun/emp_column_split \
--columns "empno,ename,job,sal,comm" \
--fields-terminated-by '\t' \
--password kobekun \
--username root \
--m 1

MySQL导入hive ***********************************

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--create-hive-table \
--hive-import \
--hive-database ruozedata_hive \
--hive-table emp_hive \
-m 1

上面操作报：ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.

sqoop 报 Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR 解决方法：
将hive 里面的lib下的hive-exec-**.jar 放到sqoop 的lib 下


create table emp_import(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
) row format delimited fields terminated  by '\t';

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--hive-overwrite \
--hive-import \
--hive-database ruozedata_hive \
--hive-table emp_import \
--fields-terminated-by '\t' \
-m 1	
	
如果不加--fields-terminated-by '\t' \的话  导入到hive中的记录都是空值null	
	
CREATE TABLE emp_import_partition(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
)
partitioned by (pt string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password ruozedata \
--username root \
--table emp \
--hive-overwrite \
--delete-target-dir \
--hive-import --hive-database ruozedata_hive \
--hive-table emp_import_partition \
--hive-partition-key 'pt' \
--hive-partition-value '2019-12-30' \
--fields-terminated-by '\t' \
-m 1
	
上面复制的老师的，执行直接报错，因为老师的文本字符编码格式是GB2312，我的是utf-8，执行下面的没问题
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--hive-overwrite \
--hive-import \
--hive-database ruozedata_hive \
--hive-table emp_import_partition \
--hive-partition-key 'pt' \
--hive-partition-value '2019-12-30' \
--fields-terminated-by '\t' \
-m 1	
	
	
hive导出到mysql**************************************
mysql中执行create table emp_demo1 as select * from emp where 1=2;

sqoop export \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table emp_demo1 \
--password kobekun \
--username root \
--export-dir /user/hive/warehouse/ruozedata_hive.db/emp_import/ \
--fields-terminated-by '\t' \
--m 1	
	
sqoop job  ********************************************	

创建sqoop job
sqoop job --create ruozedata-sqoop-job -- \
import --connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir
	
查看sqoop job	
sqoop job --list
执行
sqoop job --exec ruozedata-sqoop-job

密码明文的，如何从文件中加载密码
这个密码文件不是所有人都知道的   官网

删除job
sqoop job --delete ruozedata-sqoop-job

ETL操作****************************************************
	需求：emp和dept表是在MySQL，把MySQL数据抽取到Hive进行统计分析，
		然后将统计结果回写到MySQL中

	hive中创建emp_etl和dept_etl两张表
	统计：select e.empno,e.ename,e.deptno,e.dname from emp_etl e join dept_etl d on e.deptno=d.deptno;
	Hive: 创建结果表result_etl
	MySQL: 创建一个etl_result结果表
	
CREATE TABLE emp_etl(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE dept_etl(
deptno int,
dname string,
loc string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

	
sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table emp \
--delete-target-dir \
--hive-import \
--hive-database ruozedata_hive \
--hive-table emp_etl \
--hive-overwrite \
--fields-terminated-by '\t' \
-m 1

sqoop import \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--password kobekun \
--username root \
--table dept \
--delete-target-dir \
--hive-import \
--hive-database ruozedata_hive \
--hive-table dept_etl \
--hive-overwrite \
--fields-terminated-by '\t' \
-m 1

Hive中创建结果表
CREATE TABLE result_etl(
empno int,
ename string,
deptno int,
dname string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

insert overwrite table result_etl select  e.empno,e.ename,e.deptno,d.dname 
from emp_etl e join dept_etl d on e.deptno=d.deptno;

MySQL中创建结果表 etl_result
create table etl_result(
empno int,
ename varchar(10),
deptno int,
dname varchar(20)
);

hive中结果表导出到mysql的结果表中
sqoop export \
--connect jdbc:mysql://ruozedata001:3306/sqoop \
--table etl_result \
--password kobekun \
--username root \
--export-dir /user/hive/warehouse/ruozedata_hive.db/result_etl/ \
--fields-terminated-by '\t' \
--m 1

mysql -uroot -pkobekun <<EOF
	use sqoop;
	truncate table etl_result;
EOF




