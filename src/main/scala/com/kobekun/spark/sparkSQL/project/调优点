
1、控制文件输出的个数或者大小：coalesce
2、分区字段的数据类型调整：spark.sql.sources.partitionColumnTypeInference.enabled=false
3、批量执行插入等操作，提交使用batch操作
for(ele <- list){
        pstmt.setString(1,ele.day)
        pstmt.setLong(2,ele.cmsId)
        pstmt.setLong(3,ele.times)

        pstmt.addBatch()
      }

      pstmt.executeBatch()



mysql:
+----------+--------+--------+
| day      | cms_id | times  |
+----------+--------+--------+
| 20170511 |   4000 |  55734 |
| 20170511 |   4500 |  55366 |
| 20170511 |   4600 |  55501 |
| 20170511 |  14322 |  55102 |
| 20170511 |  14390 |  55683 |
| 20170511 |  14540 | 111027 |
| 20170511 |  14623 |  55621 |
| 20170511 |  14704 |  55701 |
+----------+--------+--------+

控制台：
+--------+-----+------+
|day     |cmsId|times |
+--------+-----+------+
|20170511|14540|111027|
|20170511|4000 |55734 |
|20170511|14704|55701 |
|20170511|14390|55683 |
|20170511|14623|55621 |
|20170511|4600 |55501 |
|20170511|4500 |55366 |
|20170511|14322|55102 |
+--------+-----+------+



